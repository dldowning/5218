{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "6bef9b2d-ca61-4a59-d369-a558142d0c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-07 04:42:30--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K)\n",
            "Saving to: ‘test.dat’\n",
            "\n",
            "\rtest.dat              0%[                    ]       0  --.-KB/s               \rtest.dat            100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-07 04:42:30 (467 MB/s) - ‘test.dat’ saved [2844/2844]\n",
            "\n",
            "--2022-02-07 04:42:30--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K)\n",
            "Saving to: ‘train.dat’\n",
            "\n",
            "train.dat           100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-07 04:42:30 (234 MB/s) - ‘train.dat’ saved [11244/11244]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the datasets\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "f39ffd09-b36b-479d-a84f-d7bd21c8a674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ],
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()[1:]   #skip index 0\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [1] + instance #appends a list with a single element 1 at the beginning of the instance list\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    #TODO: Return dot product of array 1 and array 2\n",
        "    dot_result = sum([array1[i]*array2[i] for i in range(len(array1))])   #iterates through the two lists taking the sum of the multiplied indices, needs to len of array1 since array2 has the label at the end\n",
        "    return dot_result \n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    #TODO: Return outpout of sigmoid function on x\n",
        "    sigmoid_result = (1 / (1 + math.exp(-x)))   #I had to lookup the math function instead of using numpy but I tested this works\n",
        "    return sigmoid_result\n",
        "\n",
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    #TODO: return the output of the model \n",
        "    output_result = sigmoid(dot_product(weights, instance))  #it occurs to me I could've shortened a line and put some of this in the return instead of defining a new variable, but it does add readability\n",
        "    return output_result\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    #TODO: return the prediction of the model\n",
        "    if output(weights, instance) >=.5: #simple if else statement with conditional\n",
        "      label = 1\n",
        "    else:\n",
        "      label = 0\n",
        "    return label\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    # You do not to write code like this, but get used to it\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    #TODO: name this step\n",
        "    # Initializing the weights to a 0 vector, similar to using np.zeroes with the defined size. I think in lecture we also discussed using random numbers here as a possibility, but were told it's basically as good to start with 0\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            #TODO: name these steps\n",
        "            # Multiplying the weights by the instance vector\n",
        "            # Forward step, checking the value against the known label to find the error.\n",
        "            in_value = dot_product(weights, instance)\n",
        "            output = sigmoid(in_value)\n",
        "            error = instance[-1] - output\n",
        "            #TODO: name these steps\n",
        "            # Weights update by gradient descent\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215c315c-2d2c-45e9-ef74-bda776b82c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "\n",
        "Gradient descent uses the error and the perceptron's output. Using predict returns only binary hard labels, so the error we calculate with predict will not be correct, and therefore our weight updates will also be incorrect. We need the sigmoid activation function to be used in each iteration to be able to learn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G-VKJOUu2BTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52edb5bd-fc18-4872-ea83-119081d6e14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "accuracy_list = []\n",
        "for tr_size in tr_percent:\n",
        "  best_accuracy = -1\n",
        "  for lr in lr_array:\n",
        "    for epochs in num_epochs:\n",
        "      size =  round(len(instances_tr)*tr_size/100)\n",
        "      pre_instances = instances_tr[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, instances_te)\n",
        "      if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "      print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "              f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")\n",
        "  accuracy_list.append(best_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n",
        "A) Not always, but it is a best practice when possible. Basically, more data is better but there is a diminishing return as you approach the limits of the data and the model. That makes sense theoretically, but we also see that in the above lines ran. When lr = 400 (100% of the data), we get to the 80% accuracy faster. It is possible to reach peak accuracy with a fraction of the data set with more epochs. In the simple plot below, you see that as percent of data increases so do our best accuracies. We hit peak at 80% data but that needed more epochs to reach than we needed at 100% data though both were able to hit the same accuracy.\n",
        "\n",
        "B) The learning rate is too low that the weights don't have enough iterations to be updated to better values in only 20 epochs. If we fetch the line that keeps epochs and learning rate constant while only using more training data\n",
        "\n",
        "tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
        "\n",
        "We see the accuracy of 78.0 gained. \n",
        "We can see with such a low learning rate as 0.005 that you need more epochs. But with double the data and 1/10th the learning the rate, only going to 50 epochs is enough to start seeing improved accuracy\n",
        "\n",
        "tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
        "\n",
        "C) It does not appear so. I attempted to play with the hyperparameters by hand, but I did not code a hyperparameter search iteration and let it run. At a cursory glance, it appears we are approaching an upper limit. That seems to imply it is a limitation of the model. Earlier improvements give higher gains than later improvements (with more data, more epochs, etc.) so if 80% could be exceeded then we would expect it to be diminishing returns.\n",
        "\n",
        "D) Not always, no. But it is worth it to err on the side of more epochs than fewer epochs. If you run too few, you will lose out on improved accuracy. If you run too many, then you will see repeated accuracy levels at the cost of only computing cycles. That is inefficient, so there is no reason to set epochs: 1000000 because that will be wasteful in time and energy. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(tr_percent, accuracy_list)\n",
        "plt.xlabel(\"Percent of Data\")\n",
        "plt.ylabel(\"Best Accuracy\")\n",
        "plt.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "DcFwCJ-jauur",
        "outputId": "d63686ce-a512-42ae-8853-91e49ec22447"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38rA_Kp3wiBX"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Downing HW2_The_Perceptron.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}